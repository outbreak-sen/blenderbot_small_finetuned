input question: Nice to meet you too. What are you interested in?
output answer: i ' m not really sure . i ' ve always wanted to go back to school , but i don ' t know what i want to do yet .
Map: 100%|████████████████████████████████████████████████████████████████████████████████████| 8938/8938 [00:00<00:00, 9863.37 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 9753.74 examples/s]
Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████| 245444/245444 [00:00<00:00, 2125407.75 examples/s]
Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████| 27749/27749 [00:00<00:00, 1825833.27 examples/s]
tokenizer数据集
Map:   0%|                                                                                               | 0/245444 [00:00<?, ? examples/s]/home/outbreak/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|█████████████████████████████████████████████████████████████████████████████████| 245444/245444 [07:35<00:00, 538.26 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27749/27749 [00:48<00:00, 570.94 examples/s]
Saving the dataset (2/2 shards): 100%|████████████████████████████████████████████████████████████████████████████████| 245444/245444 [00:00<00:00, 273299.21 examples/s]
Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████| 27749/27749 [00:00<00:00, 290325.56 examples/s]
/home/outbreak/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|▎                                                                                                                            | 499/184083 [00:39<4:02:00, 12.64it/s]/home/outbreak/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
  5%|▊                  | 8471/184083 [11:47<3:54:49, 12.46it/s]                                                                                                                               {'loss': 0.161, 'grad_norm': 1.2381876707077026, 'learning_rate': 3.3357701526771494e-05, 'epoch': 1.0}
{'eval_loss': 0.15160460770130157, 'eval_runtime': 90.7032, 'eval_samples_per_second': 305.932, 'eval_steps_per_second': 76.491, 'epoch': 1.0}                                                        
{'loss': 0.1269, 'grad_norm': 0.693741500377655, 'learning_rate': 1.6687683101156086e-05, 'epoch': 2.0}                                                                                               
{'eval_loss': 0.1472625583410263, 'eval_runtime': 92.5462, 'eval_samples_per_second': 299.839, 'eval_steps_per_second': 74.968, 'epoch': 2.0}                                                         
 86%|████████████████████████████████████████████▉       | 159137/184083 [3:39:00<33:24, 12.45it/s]
 86%|█████████████████████████    | 159139/184083 [3:39:00<33:26, 12.43it/s]{'loss': 0.1083, 'grad_norm': 1.1841363906860352, 'learning_rate': 1.8479968257936875e-08, 'epoch': 3.0}                                                                  
{'eval_loss': 0.1451350897550583, 'eval_runtime': 87.2231, 'eval_samples_per_second': 318.138, 'eval_steps_per_second': 79.543, 'epoch': 3.0}                             
{'train_runtime': 15213.2053, 'train_samples_per_second': 48.401, 'train_steps_per_second': 12.1, 'train_loss': 0.13209272394704427, 'epoch': 3.0}                        
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 184083/184083 [4:13:33<00:00, 12.10it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6938/6938 [01:25<00:00, 81.37it/s]
Evaluation results: {'eval_loss': 0.1451350897550583, 'eval_runtime': 85.2751, 'eval_samples_per_second': 325.406, 'eval_steps_per_second': 81.36, 'epoch': 3.0}
Traceback (most recent call last):
再次测试对话
input question: Nice to meet you too. What are you interested in?
output answer: user 2: i ' m interested in a lot of things , but my main interests are music , art , and music . i also love animals , so i volunteer at the local animal shelter to give back to the community . i have two dogs and a cat , and they ' re my whole world . user 2] user 2user 23user 1user 2 user 1 user 2 user 13user 3user 2 ' s name is user 2 , and i also have a pet iguana . i ' ve been taking care of them for a few years .
