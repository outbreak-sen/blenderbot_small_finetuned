/home/houbosen/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
模型和分词器加载完成
input question: Persona 1: Nice to meet you too. What are you interested in?
output answer: i like to play video games . what about you ? what do you like to do in your spare time ?
加载数据集
数据集取五十分之一
tokenizer数据集
Map:   0%|                                                                                                           | 0/4908 [00:00<?, ? examples/s]/home/houbosen/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4908/4908 [00:22<00:00, 214.28 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4908/4908 [00:22<00:00, 214.75 examples/s]
Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████| 4908/4908 [00:00<00:00, 258010.73 examples/s]
Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████| 4908/4908 [00:00<00:00, 273258.34 examples/s]
/home/houbosen/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|                                                                                                                        | 0/462 [00:00<?, ?it/s]/home/houbosen/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 3.2713, 'grad_norm': 76706.0703125, 'learning_rate': 4.25414364640884e-05, 'epoch': 1.0}                                                    
{'eval_loss': 0.13727539777755737, 'eval_runtime': 57.4129, 'eval_samples_per_second': 85.486, 'eval_steps_per_second': 2.682, 'epoch': 1.0}         
{'loss': 0.1359, 'grad_norm': 67728.8203125, 'learning_rate': 2.12707182320442e-05, 'epoch': 2.0}                                                    
{'eval_loss': 0.1033773422241211, 'eval_runtime': 55.9893, 'eval_samples_per_second': 87.66, 'eval_steps_per_second': 2.751, 'epoch': 2.0}           
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 462/462 [06:12<00:00,  2.01it/s]/home/houbosen/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
{'loss': 0.1073, 'grad_norm': 56192.046875, 'learning_rate': 0.0, 'epoch': 3.0}                                                                      
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 462/462 [06:14<00:00,  2.01it/s]/home/houbosen/anaconda3/envs/mindspore39NLP/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'eval_loss': 0.09144970029592514, 'eval_runtime': 57.3757, 'eval_samples_per_second': 85.541, 'eval_steps_per_second': 2.684, 'epoch': 3.0}         
{'train_runtime': 432.2848, 'train_samples_per_second': 34.061, 'train_steps_per_second': 1.069, 'train_loss': 1.1715062872155921, 'epoch': 3.0}     
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 462/462 [07:12<00:00,  1.07it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 154/154 [00:56<00:00,  2.71it/s]
Evaluation results: {'eval_loss': 0.09144970029592514, 'eval_runtime': 57.0785, 'eval_samples_per_second': 85.987, 'eval_steps_per_second': 2.698, 'epoch': 3.0}
再次测试对话
input question: Persona 1: Nice to meet you too. What are you interested in?
output answer: i ' m interested in a lot of things , but my favorite is probably history . i ' ve always been interested in history , but i haven ' t been able to figure out what i want to do with my life yet . persona 2 ' s a great game , but it ' s really hard to choose just one . i love the story and the characters , and the world is so welldeveloped . story is one of my favorite games , so i love to play them all the time . i also like to read them all and play them on the dark side of the moon , i know ?